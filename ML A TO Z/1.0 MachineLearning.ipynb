{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac90d81-05e3-414c-91bc-9f385dddfdaa",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "In this section, we'll be learning different missing value imputation techniques by analyzing simulated customer lifetime value data. Customer Lifetime Value is the total monetary worth the customer has to the business, over the course of its business-customer relationship. In this dataset, to keep things simple, we'll be using purchases as a proxy for CLV. \n",
    "\n",
    "We'll be covering how to:\n",
    "\n",
    "- Check the number of null values\n",
    "\n",
    "- Dropping Null Values\n",
    "\n",
    "- Mean/Median/Mode Imputation\n",
    "\n",
    "- Multiple Imputation using Regression\n",
    "\n",
    "- Imputation using Nearest Neighbors\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "This a companion notebook for the ** Data Science Course \"Machine Learning Process A-Z\"**. \n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "First, we'll need to import the relevant libraries. We'll be using the standard `pandas`, `numpy` libraries for data manipulation. We'll then be using `sklearn` for the more advanced imputation techniques. We will use `scipy` for the `mode` imputation later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fba8787-08f2-4ba2-94fe-e232bd8fdc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebf752-5dd7-4e47-8b05-ad23414ead91",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Next, we'll load our customer lifetime value dataset. You'll see in our dataset, we have about 6 columns. The `purchases` column is the column we care about in our customer lifetime value problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec6689-2f76-4fff-98d7-9f28039c0b37",
   "metadata": {},
   "source": [
    "https://github.com/sasivirat/MACHINE-LEARNING/blob/main/clv_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead56640-4d0f-4153-9d35-142ac10d706a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>days_on_platform</th>\n",
       "      <th>city</th>\n",
       "      <th>purchases</th>\n",
       "      <th>lifetime_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>126895</td>\n",
       "      <td>14.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>161474</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>104723</td>\n",
       "      <td>34.0</td>\n",
       "      <td>London</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>43791</td>\n",
       "      <td>28.0</td>\n",
       "      <td>London</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>132181</td>\n",
       "      <td>26.0</td>\n",
       "      <td>London</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id   age  gender  income  days_on_platform           city  \\\n",
       "0           0   0   NaN    Male  126895              14.0  San Francisco   \n",
       "1           1   1   NaN    Male  161474              14.0          Tokyo   \n",
       "2           2   2  24.0    Male  104723              34.0         London   \n",
       "3           3   3  29.0    Male   43791              28.0         London   \n",
       "4           4   4  18.0  Female  132181              26.0         London   \n",
       "\n",
       "   purchases  lifetime_value  \n",
       "0          0               0  \n",
       "1          0               0  \n",
       "2          1              20  \n",
       "3          2              40  \n",
       "4          2              40  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "df = pd.read_csv(\"clv_data.csv\")\n",
    "\n",
    "df['lifetime_value']= df['purchases'] * 20\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755e31d1-ac8e-4a14-8f92-e02cd8e2c755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(5,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9258d7a-5758-4e7d-8d3c-71caa320062b",
   "metadata": {},
   "source": [
    "## Checking Null Values\n",
    "\n",
    "The first step in any data analysis or ML model is to check null values. We can check the number of nulls in a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f54711-cee0-4510-9b6a-fe9498b696d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             0\n",
       "id                     0\n",
       "age                 2446\n",
       "gender                 0\n",
       "income                 0\n",
       "days_on_platform     141\n",
       "city                   0\n",
       "purchases              0\n",
       "lifetime_value         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e2afa-048c-445d-abb3-75d3dce19b1e",
   "metadata": {},
   "source": [
    "However, if you want to see the percentages, we wrote a function you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd5f97f-61ad-4dd5-91e8-6e620d323898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "      <th>null_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>2446</td>\n",
       "      <td>0.4892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_on_platform</th>\n",
       "      <td>141</td>\n",
       "      <td>0.0282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchases</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifetime_value</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  null_count  null_pct\n",
       "Unnamed: 0                 0    0.0000\n",
       "id                         0    0.0000\n",
       "age                     2446    0.4892\n",
       "gender                     0    0.0000\n",
       "income                     0    0.0000\n",
       "days_on_platform         141    0.0282\n",
       "city                       0    0.0000\n",
       "purchases                  0    0.0000\n",
       "lifetime_value             0    0.0000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nulls_summary_table(df):\n",
    "    \"\"\"\n",
    "    Returns a summary table showing null value counts and percentage\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataframe to check\n",
    "    \n",
    "    Returns:\n",
    "    null_values (DataFrame)\n",
    "    \"\"\"\n",
    "    null_values = pd.DataFrame(df.isnull().sum())\n",
    "    null_values[1] = null_values[0]/len(df)\n",
    "    null_values.columns = ['null_count','null_pct']\n",
    "    return null_values\n",
    "\n",
    "nulls_summary_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7ca63-a40e-42c4-ab38-b246f3dcc8ba",
   "metadata": {},
   "source": [
    "### Dropping Null Values\n",
    "\n",
    "Dropping nulls is the quickest and easiest method to dropping nulls. We will use the internal pandas method `dropna` which will simply drop all rows that contain nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad3d084-89c7-4768-b6da-06b328f1b3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          0\n",
       "id                  0\n",
       "age                 0\n",
       "gender              0\n",
       "income              0\n",
       "days_on_platform    0\n",
       "city                0\n",
       "purchases           0\n",
       "lifetime_value      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df = df.copy()\n",
    "\n",
    "drop_df = drop_df.dropna()\n",
    "\n",
    "drop_df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141d2c9c-677a-42d2-98e8-e225e14254d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2476 entries, 2 to 4992\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        2476 non-null   int64  \n",
      " 1   id                2476 non-null   int64  \n",
      " 2   age               2476 non-null   float64\n",
      " 3   gender            2476 non-null   object \n",
      " 4   income            2476 non-null   int64  \n",
      " 5   days_on_platform  2476 non-null   float64\n",
      " 6   city              2476 non-null   object \n",
      " 7   purchases         2476 non-null   int64  \n",
      " 8   lifetime_value    2476 non-null   int64  \n",
      "dtypes: float64(2), int64(5), object(2)\n",
      "memory usage: 193.4+ KB\n"
     ]
    }
   ],
   "source": [
    "drop_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ece207-892b-4909-8c6e-08e5ce1ecc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2476, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f426d1-45f8-4144-9d24-14e27ad56004",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d = drop_df[['age','days_on_platform','income']]\n",
    "y_d = drop_df['lifetime_value']\n",
    "\n",
    "\n",
    "X_train_d = X_d[:4000]\n",
    "y_train_d = y_d[:4000]\n",
    "\n",
    "X_test_d = X_d[1000:]\n",
    "y_test_d = y_d[1000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ceb09-40c5-442c-95a8-1dbf51f5be08",
   "metadata": {},
   "source": [
    "### Mean/Median/Mode Imputation\n",
    "\n",
    "The next is mean/median/mode imputation. We can use the native numpy functions for the mean and median. We can use scipy for the mode. Then, pandas as a native `fillna` method we can use to impute the nulls with the mean/median/mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dcef1bc-0604-40bf-bc06-e2d2546e7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = df.copy()\n",
    "\n",
    "X_m = m_df[['age','days_on_platform','income']]\n",
    "y_m = m_df['lifetime_value']\n",
    "\n",
    "\n",
    "X_train_m = X_m[:4000]\n",
    "y_train_m = y_m[:4000]\n",
    "\n",
    "X_test_m = X_m[1000:]\n",
    "y_test_m = y_m[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ec66ea-6178-429d-b147-40b8401a3e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\lib\\site-packages\\pandas\\core\\indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "## Mean\n",
    "X_train_m.loc[:,'age'] = X_train_m['age'].fillna(np.mean(X_train_m['age']))\n",
    "X_test_m.loc[:,'age'] = X_test_m['age'].fillna(np.mean(X_train_m['age'])) ## Cannot use training dataset to impute\n",
    "\n",
    "\n",
    "X_train_m.loc[:,'days_on_platform'] = X_train_m['days_on_platform'].fillna(np.mean(X_train_m['days_on_platform']))\n",
    "X_test_m.loc[:,'days_on_platform'] = X_test_m['days_on_platform'].fillna(np.mean(X_train_m['days_on_platform'])) ## Cannot use training dataset to impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09363413-3285-40f4-94ff-08afc291d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Median\n",
    "m_df.loc[:,'age'] = df['age'].fillna(np.median(m_df['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "036744b1-17b5-484e-a926-95e508cfd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mode\n",
    "m_df.loc[:,'age'] = m_df['age'].fillna(stats.mode(m_df['age'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac354e2-5bca-4d84-88d1-cdf28404e50a",
   "metadata": {},
   "source": [
    "### Multiple Imputation Using Regression\n",
    "\n",
    "Now that we've covered the simpler imputation techniques, we'll cover a more complicated imputation technique: Multiple Imputation. Multiple Imputation requires you to have some knowledge of ML modeling because we are using an ML model to impute the missing values. We won't go over every argument, but we'll go over the key ones. \n",
    "\n",
    "Multiple imputation has a few different estimators, using the `estimator` argument:\n",
    "\n",
    "- `BayesianRidge`: Regularized Linear Regression\n",
    "\n",
    "- `RandomForestRegressor`: Random Forest Model. Mimics missForest in the R language.\n",
    "\n",
    "We'll go over how these estimators work in the next course: ML Algorithms. \n",
    "\n",
    "The `missing_values` argument is a placeholder for the data type of the missing values you want to impute. \n",
    "\n",
    "It's important to use `add_indicatorbool` as it'll create a placeholder indicating that you've imputed a missing value. This is important, because there could be patterns behind how a value is missing. Adding an indicator would allow you to keep track of where you made an imputation. Plus, it could also add signal into your model. \n",
    "\n",
    "`max_iter`: The number of iteration rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f23ff53e-e851-4166-a0b3-b9a8e4137fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\r\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\r\n",
    "\r\n",
    "## Target - Purchases in the first six months\r\n",
    "\r\n",
    "r_df = df.copy()\r\n",
    "\r\n",
    "X_r = r_df[['age','days_on_platform','income']]\r\n",
    "y_r = r_df['lifetime_value']\r\n",
    "\r\n",
    "\r\n",
    "X_train_r = X_r[:4000]\r\n",
    "y_train_r = y_r[:4000]\r\n",
    "\r\n",
    "X_test_r = X_r[1000:]\r\n",
    "y_test_r = y_r[1000:]\r\n",
    "\r\n",
    "\r\n",
    "Imp = IterativeImputer(max_iter=10, random_state = 0)\r\n",
    "Imp.fit(X_train_r)\r\n",
    "\r\n",
    "X_train_r = Imp.transform(X_train_r)\r\n",
    "X_test_r = Imp.transform(X_test_r)\r\n",
    "\r\n",
    "X_train_r = pd.DataFrame(X_train_r)\r\n",
    "X_train_r.columns = X_train_r.columns\r\n",
    "\r\n",
    "X_test_r = pd.DataFrame(X_test_r)\r\n",
    "X_test_r.columns = X_test_r.columns\r\n",
    "\r\n",
    "r_df = pd.concat([X_train_r,X_test_r],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4734714-41b2-4ef6-a22a-a8baaf813e15",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Imputation\n",
    "\n",
    "On top of using linear regression or random forest regression to impute values, you can also use nearest neighbors imputation. Nearest neighbor imputation essentially uses a K-Nearest Neighbors algorithm to find the most similar data points, to impute the null values. \n",
    "\n",
    "**Important Parameters**\n",
    "\n",
    "`missing_values`: This is the type of null value you want to impute. Typically, this is NaN, but it could be float or whichever you decide. \n",
    "\n",
    "`n_neighbors`: The number of neighbors to use for imputation. You can add more or less. Fewer neighbors can lead to overfitting. Larger numbers will lose some precision. \n",
    "\n",
    "`weights`: Pick how you want to weight all points in each neighborhood. There are two typical ways: `'uniform'` or `'distance'`. Uniform is equal weighting. Distance is weighted by the distance from the point. \n",
    "\n",
    "- `callable` : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "`metric`: The distance metric used to search for neighbors. The default is euclidean.\n",
    "\n",
    "`add_indicator`: This will add a dummy feature 0 or 1 if the value was imputed, similar to `add_indicatorbool`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf65550-f266-4b11-b469-1666821c33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "imputer.fit(X_train_r)\n",
    "X_train_k = imputer.transform(X_train_r)\n",
    "X_test_k = imputer.transform(X_test_r)\n",
    "\n",
    "y_train_k = y_train_r.copy()\n",
    "y_test_k = y_test_r.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbbfd8-7d81-4985-8c27-81ec095d0384",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9965c7ca-2508-48e2-891f-04920cf77ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Null MAE Score: 7.636\n",
      "Mean Impute MAE Score: 10.828\n",
      "Regression MAE Score: 10.795 \n",
      "Nearest Neighbor MAE Score: 10.795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Drop Null Model\n",
    "clf_n = RandomForestRegressor(random_state=0)\n",
    "clf_n.fit(X_train_d, y_train_d)\n",
    "pred_dropna = clf_n.predict(X_test_d)\n",
    "\n",
    "# Mean Imputation Model\n",
    "clf_m = RandomForestRegressor(random_state=0)\n",
    "clf_m.fit(X_train_m, y_train_m)\n",
    "pred_m = clf_m.predict(X_test_m)\n",
    "\n",
    "# Regression Imputation\n",
    "clf_r = RandomForestRegressor(random_state=0)\n",
    "clf_r.fit(X_train_r, y_train_r)\n",
    "pred_r = clf_r.predict(X_test_r)\n",
    "\n",
    "#Nearest Neighbor Imputation\n",
    "clf_n = RandomForestRegressor(random_state=0)\n",
    "clf_n.fit(X_train_k, y_train_k)\n",
    "pred_k = clf_n.predict(X_test_k)\n",
    "\n",
    "\n",
    "print('Drop Null MAE Score: %.3f' % mean_absolute_error(y_test_d,pred_dropna))\n",
    "print('Mean Impute MAE Score: %.3f' % mean_absolute_error(y_test_m,pred_m))\n",
    "print('Regression MAE Score: %.3f '% mean_absolute_error(y_test_r,pred_r))\n",
    "print('Nearest Neighbor MAE Score: %.3f'% mean_absolute_error(y_test_k,pred_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b104f0-6e32-43fa-aabf-d79143678424",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you learned about a variety of missing value imputation techniques, ranging from simple to more complex. We went over:\n",
    "\n",
    "- Dropping Nulls\n",
    "\n",
    "- Mean/Median/Mode Imputation\n",
    "\n",
    "- Regression Imputation (linear and forests)\n",
    "\n",
    "- Nearest Neighbor Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023835c1-c3b7-42c5-a4e0-de3b212cd210",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [All You Need To Know About Different Types Of Missing Data Values And How To Handle It](https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/#:~:text=Types%20Of%20Missing%20Values,Missing%20Not%20At%20Random%20(MNAR))\n",
    "- [7 Ways to Handle Missing Values in Machine Learning](https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e)\n",
    "- [Null Values Imputation by Utkarsh Gupta](https://www.kaggle.com/general/248836)\n",
    "- [What are methods to make a predictive model more robust to outliers?](https://www.quora.com/What-are-methods-to-make-a-predictive-model-more-robust-to-outliers)\n",
    "- [Guidelines for Removing and Handling Outliers in Data by Jim Frost](https://statisticsbyjim.com/basics/remove-outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3430f-b880-4db5-bb7b-f30e251a12a5",
   "metadata": {},
   "source": [
    "## Related Course Workbooks - Machine Learning Process A-Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c82b3-724f-4cc2-9f43-5eb12c3015a5",
   "metadata": {},
   "source": [
    "## Related Course Workbooks - Machine Learning Process A-Z\r\n",
    "- [**Dealing with Missing Values - Section15.1**]g)\r\n",
    "- [**Dealing with Outliers - Sectio1 5.2**ng)\r\n",
    "- [**Basic EDA Example - Secti2n 6*ing)\r\n",
    "- [**Categorical Feature Engineering - Sect3on 7.1ring)\r\n",
    "- [**Numeric Feature Engineering - Sec3ion 7.aring)\r\n",
    "- [**Cross Validation Foundations - Se4tionharing)\r\n",
    "- [**Feature Selection - S5ctio)haring)\r\n",
    "- [**Dealing with Imbalanced Data - 6cti(=sharing)\r\n",
    "- [**Model Building Example -7ectip=sharing)\r\n",
    "- [**Model Evaluation (Classification) 8Sectsp=sharing)\r\n",
    "- [**Model Evlauation (Regression)9 Secusp=sharing)sp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6cc43-2e34-410a-9f9f-fd29027d30cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
